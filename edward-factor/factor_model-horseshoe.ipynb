{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian PCA\n",
    "Includes bias and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from edward.models import Normal, StudentT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, tf.__path__[0] + '/contrib/distributions/python/ops')\n",
    "\n",
    "import bijectors as bijector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.          0.        ]\n",
      " [-1.          0.5         0.        ]\n",
      " [ 0.         -0.5         0.33333333]\n",
      " [ 0.          0.         -0.33333333]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# note that in the function below D should be greater then K\n",
    "def build_toy_dataset(N, D, K):\n",
    "    x_train = np.zeros([D, N])\n",
    "    w = np.zeros([D,K])\n",
    "    for k in range(K):\n",
    "        w[k,k]=1.0/(k+1)\n",
    "        w[k+1,k]=-1.0/(k+1)\n",
    "    print(w)\n",
    "    z = np.random.normal(0.0, 1.0, size=(K, N))\n",
    "    mean = np.dot(w, z)\n",
    "    shift=np.zeros([D])\n",
    "    shift[0]=10\n",
    "    shift[1]=23\n",
    "    sigma = np.ones(D)+0.1\n",
    "    for d in range(D):\n",
    "      for n in range(N):\n",
    "        x_train[d, n] = np.random.normal(mean[d, n], sigma[d])+shift[d]\n",
    "#     print(\"True principal axes:\")\n",
    "#     print(w)\n",
    "#     print(\"Shift:\")\n",
    "#     print(shift)\n",
    "    return x_train.astype(np.float32,copy=False), w, shift, sigma\n",
    "\n",
    "\n",
    "#ed.set_seed(142)\n",
    "\n",
    "N = 1000  # number of data points\n",
    "D = 6  # data dimensionality\n",
    "K = 3 # latent dimensionality\n",
    "\n",
    "# DATA\n",
    "\n",
    "x_train, w_true, shift, sigma_true = build_toy_dataset(N, D, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.,  23.,   0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.1,  1.1,  1.1,  1.1,  1.1,  1.1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a diagonal noise covariance matrix\n",
    "\n",
    "So far we used the knowledge on the dimensionality of the latent space by setting $K=1$.  We can determine the number of latent dimensions by setting a higher $K$ (in this case $K=2$) and adding another prior $\\alpha=(\\alpha_1,\\dots,\\alpha_K)$ for the scale of each dimension.\n",
    "\\begin{align}\n",
    "\\alpha_j&\\sim Gamma(1,1)\\\\\n",
    "w_{ij}&\\sim N(0,\\alpha_j)\\\\\n",
    "z_j&\\sim N(0,1)\\\\\n",
    "\\mu_i &\\sim N(0,1)\\\\\n",
    "\\sigma_i &\\sim Gamma(1,1)\\\\\n",
    "x_i&\\sim N((w\\cdot z)_i+\\mu_i,\\sigma_i)\\\\\n",
    "\\end{align}\n",
    "\n",
    "We expect the posterior for either $\\alpha_1$ or $\\alpha_2$, say $\\alpha_2$, to drop down to zero, signaling that there are is only one significant dimension left, $\\alpha_1\\ne0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The automatic relevance determination in higher dimensions\n",
    "\n",
    "The ARD might seem like an overkill but it comes in handy in higher dimensions, where the dimensionality of the latent space is not a prior known.  Let us generate a higher dimensional data set and see how ARD determines the latent dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class low_rank_mvn_horseshoe_ard():\n",
    "    def __init__(self,M,D):\n",
    "        # Fit as many latent variables as dimensions, prune by ARD\n",
    "        K=D\n",
    "        #########\n",
    "        # halfCauchy = StudentT(df=1., loc=0., scale=1.)\n",
    "        # Horseshoe! Happy now, Aki? Hyva!\n",
    "        #########\n",
    "        # P-model\n",
    "        sigma = ed.models.Gamma(tf.ones([D]),tf.ones([D]))\n",
    "        #alpha = ed.models.Gamma(tf.ones([K]),tf.ones([K]))\n",
    "        lamda = StudentT(df=1., loc=tf.zeros([K]), scale=tf.ones([K]))\n",
    "        tau = StudentT(df=1., loc=0., scale=1.)\n",
    "        w = Normal(tf.zeros([D, K]),    tf.reshape(tf.tile(np.abs(tau)*np.abs(lamda),[D]),[D,K]))\n",
    "        z = Normal(tf.zeros([K, M]),    tf.ones([K, M]))\n",
    "        mu = Normal(tf.zeros([D]),       tf.ones([D]))\n",
    "        x = ed.models.MultivariateNormalDiag(tf.add(tf.transpose(tf.matmul(w, z)),mu), tf.reshape(tf.tile(sigma,[M]),[M,D]))\n",
    "\n",
    "        self.Pmodel = (sigma,lamda,tau,w,z,mu)\n",
    "        self.x = x\n",
    "        #########\n",
    "        # Q-model\n",
    "        qtau = ed.models.TransformedDistribution(\n",
    "        distribution=ed.models.NormalWithSoftplusScale(tf.Variable(tf.random_normal([])),\n",
    "                                                       tf.Variable(tf.random_normal([]))),\n",
    "        bijector=bijector.Exp(),\n",
    "        name=\"qtau\")\n",
    "        qlamda = ed.models.TransformedDistribution(\n",
    "        distribution=ed.models.NormalWithSoftplusScale(tf.Variable(tf.zeros([K])),\n",
    "                                                       tf.Variable(tf.zeros([K]))),\n",
    "        bijector=bijector.Exp(),\n",
    "        name=\"qlamda\")\n",
    "        #qalpha = ed.models.TransformedDistribution(\n",
    "        #    distribution=ed.models.NormalWithSoftplusScale(tf.Variable(tf.random_normal([K])),\n",
    "        #                                                   tf.Variable(tf.random_normal([K]))),\n",
    "        #    bijector=bijector.Exp(),\n",
    "        #    name=\"qalpha\")\n",
    "        qw = Normal(tf.Variable(tf.random_normal([D, K])),\n",
    "                    tf.nn.softplus(tf.Variable(tf.random_normal([D, K]))))\n",
    "        qz = Normal(tf.Variable(tf.random_normal([K, M])),\n",
    "                    tf.nn.softplus(tf.Variable(tf.random_normal([K, M]))))\n",
    "\n",
    "        # assume standardized data\n",
    "        qmu = Normal(tf.Variable(tf.random_normal([D])),\n",
    "                    tf.nn.softplus(tf.Variable(tf.random_normal([D]))))\n",
    "        qsigma = ed.models.TransformedDistribution(\n",
    "            distribution=ed.models.NormalWithSoftplusScale(tf.Variable(tf.zeros(D)),\n",
    "                                          tf.Variable(tf.ones(D))),\n",
    "            bijector=bijector.Exp(),\n",
    "            name=\"qsigma\")\n",
    "\n",
    "        self.Qmodel = (qsigma,qlamda,qtau,qw,qz,qmu)\n",
    "        \n",
    "    def initialize(self, x_train):\n",
    "        '''\n",
    "        Initialize parameters of Q-model in the solution from PCA, and empirical means and standard deviations, for faster convergence.\n",
    "        '''\n",
    "        K=D\n",
    "        qsigma,qlamda,qtau,qw,qz,qmu = self.Qmodel\n",
    "        # set qmu mean to data mean\n",
    "        data_mean = np.mean(x_train,axis=1).astype(np.float32,copy=False)\n",
    "        qmu = Normal(tf.Variable(data_mean+tf.random_normal([D])),\n",
    "                    tf.nn.softplus(tf.Variable(tf.random_normal([D]))))\n",
    "        # set qw mean to pca solution\n",
    "        U,_,_ = np.linalg.svd(x_train)\n",
    "        qw = Normal(tf.Variable(U+tf.random_normal([D, K])),\n",
    "                    tf.nn.softplus(tf.Variable(tf.random_normal([D, K]))))\n",
    "        # set qsigma mean to data stds\n",
    "        data_std = np.std(x_train,axis=1).astype(np.float32,copy=False)\n",
    "        qsigma = ed.models.TransformedDistribution(\n",
    "            distribution=ed.models.NormalWithSoftplusScale(tf.Variable(np.log(data_std)),\n",
    "                                          tf.Variable(tf.ones(D)*3)),\n",
    "            bijector=bijector.Exp(),\n",
    "            name=\"qsigma\")\n",
    "        self.Qmodel = (qsigma,qlamda,qtau,qw,qz,qmu)\n",
    "\n",
    "    def infer(self, x_train, n_epoch = 100, n_print=100, n_samples=100, M = 100, optimizer='rmsprop'):\n",
    "        # add mini-batches\n",
    "        def generator(arrays, batch_size):\n",
    "            \"\"\"Generate batches, one with respect to each array's first axis.\"\"\"\n",
    "            starts = [0] * len(arrays)  # pointers to where we are in iteration\n",
    "            while True:\n",
    "                batches = []\n",
    "                for i, array in enumerate(arrays):\n",
    "                    start = starts[i]\n",
    "                    stop = start + batch_size\n",
    "                    diff = stop - array.shape[0]\n",
    "                    if diff <= 0:\n",
    "                        batch = array[start:stop]\n",
    "                        starts[i] += batch_size\n",
    "                    else:\n",
    "                        batch = np.concatenate((array[start:], array[:diff]))\n",
    "                        starts[i] = diff\n",
    "                    batches.append(batch)\n",
    "                yield batches\n",
    "        x_ph = tf.placeholder(tf.float32, [None, D])\n",
    "        data = generator([x_train.T], M)\n",
    "        n_batch = int(N / M)\n",
    "        # add progress bar\n",
    "        x = self.x\n",
    "        inference = ed.KLqp(dict(zip(self.Pmodel,self.Qmodel)), data={x: x_ph})\n",
    "        inference.initialize(n_iter=n_batch * n_epoch, n_print=(n_batch * n_epoch)/10, n_samples=n_samples)\n",
    "\n",
    "        sess = ed.get_session()\n",
    "        init = tf.global_variables_initializer()\n",
    "        init.run()\n",
    "\n",
    "        # assert x_ph.shape[0] == M \n",
    "        learning_curve = []\n",
    "        for _ in range(inference.n_iter):\n",
    "            x_batch = next(data)[0]\n",
    "            info_dict = inference.update({x_ph: x_batch})\n",
    "            if _%inference.n_print == 0:\n",
    "                print(info_dict)\n",
    "            learning_curve.append(info_dict['loss'])\n",
    "        plt.semilogy(learning_curve)\n",
    "        plt.show()\n",
    "    \n",
    "    def prior_predictive_check(self):\n",
    "        self.x_prior = ed.copy(self.x)\n",
    "        pass\n",
    "        \n",
    "    def posterior_predictive_check(self, x_test):\n",
    "        self.x_post = ed.copy(self.x, dict(zip(self.Pmodel,self.Qmodel)))\n",
    "        return ed.evaluate('log_likelihood', data={self.x_post: x_test})\n",
    "    \n",
    "    def print_model(self):\n",
    "        qsigma,qlamda,qtau,qw,qz,qmu = self.Qmodel\n",
    "        # add pair plots\n",
    "        print(\"Inferred principal axes (columns):\")\n",
    "        print(qw.mean().eval())\n",
    "        print(qw.variance().eval())\n",
    "        print(\"Inferred center:\")\n",
    "        print(qmu.mean().eval())\n",
    "        print(qmu.variance().eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M = 1000\n",
    "meddle = low_rank_mvn_horseshoe_ard(M,D)\n",
    "meddle.initialize(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meddle.prior_predictive_check()\n",
    "\n",
    "# sample all variables ancestrally\n",
    "model = meddle.x_prior.get_ancestors()\n",
    "model.append(meddle.x_prior)\n",
    "model_sample = dict(zip(model,sess.run([v.value() for v in model])))\n",
    "\n",
    "sess = ed.get_session()\n",
    "tmp = meddle.x_prior.sample().eval().T\n",
    "with sess.as_default():\n",
    "    plt.scatter(*tmp)\n",
    "    plt.scatter(*x_train[:,:M])\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meddle.infer(x_train, M=M, n_epoch = 200, n_samples = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meddle.posterior_predictive_check(x_train[:,:M].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_post_mode = meddle.x_post.mode().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_against_training_set(x):\n",
    "    x_train_sample = x_train[:,:M]\n",
    "    for i in range(D-2):\n",
    "        plt.scatter(*x_train_sample[i:i+2,:])\n",
    "        plt.scatter(*x.T[i:i+2,:],alpha=.1)\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "    plt.scatter(*x_train_sample[(D-2):D,:])\n",
    "    plt.scatter(*x.T[(D-2):D,:],alpha=.1)\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_against_training_set(x_post_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinton(matrix, max_weight=None, ax=None):\n",
    "    \"\"\"Draw Hinton diagram for visualizing a weight matrix.\"\"\"\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "\n",
    "    if not max_weight:\n",
    "        max_weight = 2 ** np.ceil(np.log(np.abs(matrix).max()) / np.log(2))\n",
    "\n",
    "    ax.patch.set_facecolor('gray')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    for (x, y), w in np.ndenumerate(matrix):\n",
    "        color = 'white' if w > 0 else 'black'\n",
    "        size = np.sqrt(np.abs(w) / max_weight)\n",
    "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
    "                             facecolor=color, edgecolor=color)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.autoscale_view()\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOgAAADuCAYAAAAz8EtlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTEAhmIBCMAGFYP8C5PwKrSaftn8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb707604290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w_hat = meddle.Qmodel[3].mean().eval()\n",
    "hinton(w_hat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.20999999,  1.16999996,  0.01      ,  0.79000002,  0.50999999,\n",
       "        -0.61000001],\n",
       "       [-2.98000002, -0.77999997,  2.5       , -0.03      , -0.76999998,\n",
       "        -0.40000001],\n",
       "       [-0.41      ,  0.63999999,  0.86000001,  1.23000002,  0.20999999,\n",
       "         0.58999997],\n",
       "       [-0.23999999,  0.05      , -0.94999999, -2.1099999 , -0.15000001,\n",
       "         0.72000003],\n",
       "       [-0.94999999, -0.2       , -0.38      ,  0.2       , -2.33999991,\n",
       "        -1.11000001],\n",
       "       [-0.19      ,  0.63      , -0.23      ,  0.2       , -0.03      ,\n",
       "         2.1500001 ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(w_hat,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ,  0.        ],\n",
       "       [-1.        ,  0.5       ,  0.        ],\n",
       "       [ 0.        , -0.5       ,  0.33333333],\n",
       "       [ 0.        ,  0.        , -0.33333333],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATwAAADuCAYAAACkhLuWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAABIBJREFUeJzt3bFu22YARlG68BRq8BJn7OMFyCpo\n927oITMZSDtSM/sEqSNHzC/pnjMT5MflggQI8GFd1wmg4K/RAwD+FMEDMgQPyBA8IEPwgAzBAzIE\nD8gQPCBD8ICMx3MO/vTp0/r09LTVFoAPeXt7+3dd18/vHXdW8J6enqavX79+fBXABl5eXr7/ynFe\naYEMwQMyBA/IEDwgQ/CADMEDMgQPyBA8IOOsD48/Yr/fT7vdbuvL3KQvX75MP378GD3jKs3zPB0O\nh9EzuDObP+GJ3c+J3c+dTqfRE7hDXmmBDMEDMgQPyBA8IEPwgAzBAzIED8gQPCBD8IAMwQMyBA/I\nEDwgQ/CADMEDMgQPyBA8IEPwgAzBAzIED8gQPCBD8IAMwQMyBA/IEDwgQ/CADMEDMgQPyBA8IEPw\ngAzBAzIED8gQPCBD8IAMwQMyBA/IEDwgQ/CADMEDMgQPyBA8IEPwgAzBAzIED8gQPCBD8IAMwQMy\nBA/IEDwgQ/CADMEDMgQPyBA8IEPwgAzBAzIED8gQPCBD8IAMwQMyBA/IEDwgQ/CADMEDMgQPyBA8\nIEPwgAzBAzIED8gQPCBD8ICMzYO3LMvWl7hZz8/PoydcrXmeR0/gDj1ufYHj8bj1JW7Wt2/fRk+A\nFK+0QIbgARmCB2QIHpAheECG4AEZggdkCB6QsfmHx/y+/X4/7Xa70TMuYlmWi36M/vr6Op1Op4ud\nb5R5nqfD4TB6xt3zhHcD7iV203T5e7mH2E3T/dzHtRM8IEPwgAzBAzIED8gQPCBD8IAMwQMyBA/I\nEDwgQ/CADMEDMgQPyBA8IEPwgAzBAzIED8gQPCBD8IAMwQMyBA/IEDwgQ/CADMEDMgQPyBA8IEPw\ngAzBAzIED8gQPCBD8IAMwQMyBA/IEDwgQ/CADMEDMgQPyBA8IEPwgAzBAzIED8gQPCBD8IAMwQMy\nBA/IEDwgQ/CADMEDMgQPyBC8G7Asy+gJF3Ppe5nn+aLnG+Ve7uPaPY4ewPuOx+PoCVfrcDiMnsAN\n8YQHZAgekCF4QIbgARmCB2QIHpAheECG4AEZggdkCB6QIXhAhuABGYIHZAgekCF4QIbgARmCB2QI\nHpAheECG4AEZggdkCB6QIXhAhv/Scnf2+/202+1Gz/hfy7IM+d/w6+vrdDqd/vh1zzHP82b/G/aE\nx9259thN07iN1x67adp2o+ABGYIHZAgekCF4QIbgARmCB2QIHpAheECG4AEZggdkCB6QIXhAhuAB\nGYIHZAgekCF4QIbgARmCB2QIHpAheECG4AEZggdkCB6QIXhAhuABGYIHZAgekCF4QIbgARmCB2QI\nHpAheECG4AEZggdkCB6QIXhAhuABGYIHZAgekCF43J1lWUZPeNeojfM8D7nuObbc+LjZmWGQ4/E4\nesLVOhwOoycM5QkPyBA8IEPwgAzBAzIED8gQPCBD8IAMwQMyHtZ1/fWDHx7+mabp+3ZzAD7k73Vd\nP7930FnBA7hlXmmBDMEDMgQPyBA8IEPwgAzBAzIED8gQPCBD8ICM/wBdp2OYXnMY+gAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb707b6d890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hinton(w_true)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qsigma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7e5c737b52cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mqsigma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqsigma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qsigma' is not defined"
     ]
    }
   ],
   "source": [
    "qsigma.get_variables()[0].eval()\n",
    "\n",
    "sorted(np.exp(qsigma.get_variables()[0].eval()))\n",
    "\n",
    "tmp = qw.mean().eval()\n",
    "\n",
    "tmp[abs(tmp)>0.05]\n",
    "\n",
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas=tf.exp(qalpha.distribution.mean()).eval()\n",
    "alphas.sort()\n",
    "plt.plot(range(alphas.size),alphas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(qalpha.sample(1000).eval(),bins=20)\n",
    "plt.xlim(0,2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qsigma.get_variables()[0].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sim_sample = x_sim.sample().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sim_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sim_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = ed.get_session()\n",
    "init = tf.global_variables_initializer()\n",
    "init.run()\n",
    "tpm = qtmp.sample(10000).eval()\n",
    "\n",
    "plt.hist(tpm[:,0],'auto',normed=True)\n",
    "plt.axis([0,50,0,1])\n",
    "plt.show()\n",
    "plt.hist(np.log(tpm[:,0]),'auto',normed=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
